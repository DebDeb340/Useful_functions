{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Python Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is created to gather Python functions used during Data Analyst training between October 2023 and September 2024.\n",
    "This might cover anything from going through files to machine learning functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ethics - Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Anonymisation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Déborah Leclercq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Library needed for this function to work:\n",
    "- library anonypy to anonymise data\n",
    "- import anonypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymisator(df, categorical, cols_to_anonymise, cols_to_keep, sensitive_columns, k):\n",
    "    ''' \n",
    "    Anonymize specific columns of a dataframe based on k-anonymity.\n",
    "    Ensure you install library called anonypy\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The original dataframe.\n",
    "    categorical (list): List of categorical column names.\n",
    "    cols_to_anonymise (list): List of columns that need to be anonymised.\n",
    "    cols_to_keep (list): Columns that should not be modified or anonymized.\n",
    "    sensitive_columns (list): Columns with sensitive information that shouldn't be modified.\n",
    "    k (int): Level of anonymization for k-anonymity.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A dataframe with anonymized data.\n",
    "    '''\n",
    "\n",
    "    # Ensure the categorical columns are correctly set as category dtype\n",
    "    for name in categorical:\n",
    "        df[name] = df[name].astype(\"category\")\n",
    "\n",
    "    # Create a unique identifier (rownumber) for merging later\n",
    "    df['rownumber'] = df.index\n",
    "\n",
    "    # Initialize the anonymizer with the relevant columns\n",
    "    p = anonypy.Preserver(df, cols_to_anonymise, cols_to_keep + sensitive_columns)\n",
    "\n",
    "    # Perform the k-anonymity anonymization\n",
    "    rows = p.anonymize_k_anonymity(k=k)\n",
    "\n",
    "    # Convert the anonymized rows into a new DataFrame\n",
    "    df_anonyme = pd.DataFrame(rows)\n",
    "\n",
    "    # Drop the 'count' column created during anonymization if it exists\n",
    "    if 'count' in df_anonyme.columns:\n",
    "        df_anonyme = df_anonyme.drop(columns=['count'])\n",
    "\n",
    "    # Merge anonymized data back with the original dataframe on 'rownumber'\n",
    "    dataset_anonymised = df.drop(columns=cols_to_anonymise).merge(df_anonyme, how='left', on='rownumber')\n",
    "\n",
    "    # Drop the 'rownumber' column as it's no longer needed\n",
    "    dataset_anonymised = dataset_anonymised.drop(columns=['rownumber'])\n",
    "\n",
    "    return dataset_anonymised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Déborah Leclercq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Libraries needed for the next function:\n",
    "- from sklearn.preprocessing import StandardScaler\n",
    "- from sklearn.model_selection import train_test_split\n",
    "- from sklearn.neighbors import KNeighborsClassifier\n",
    "- from sklearn.metrics import confusion_matrix,  precision_score, recall_score, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to train and test after anonymising data\n",
    "def preprocessor(df, multicategorical_features, continuous_features, target='target', stratify_col='sex', test_size=0.3, random_state=41):\n",
    "    '''\n",
    "    Preprocesses the data, trains a KNN classifier, and evaluates performance.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input dataframe.\n",
    "    - multicategorical_features (list): List of multi-categorical feature columns.\n",
    "    - continuous_features (list): List of continuous feature columns to scale.\n",
    "    - target (str): Target column name.\n",
    "    - stratify_col (str): Column to use for stratification (e.g., 'sex').\n",
    "    - test_size (float): Proportion of test data. Default is 0.3.\n",
    "    - random_state (int): Random state for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary of evaluation metrics.\n",
    "    '''\n",
    "\n",
    "    # One-hot encoding for multi-categorical features\n",
    "    dataset = pd.get_dummies(df, columns=multicategorical_features)\n",
    "    \n",
    "    # Separate features (X) and target (y)\n",
    "    y = dataset[target]\n",
    "    X = dataset.drop([target], axis=1)\n",
    "    \n",
    "    # Splitting data into train and test sets with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, random_state=random_state, \n",
    "        test_size=test_size, stratify=df[stratify_col]  # Only stratifying based on one column\n",
    "    )\n",
    "\n",
    "    # Scaling continuous features\n",
    "    standardScaler = StandardScaler()\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    \n",
    "    X_train_scaled[continuous_features] = standardScaler.fit_transform(X_train[continuous_features])\n",
    "    X_test_scaled[continuous_features] = standardScaler.transform(X_test[continuous_features])\n",
    "    \n",
    "    # Training KNN classifier with k=15\n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=15)\n",
    "    knn_clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_test_pred = knn_clf.predict(X_test_scaled)\n",
    "    \n",
    "    # Returning evaluation metrics\n",
    "    return {\n",
    "        'accuracy_score': accuracy_score(y_test, y_test_pred),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_test_pred),\n",
    "        'precision_score': precision_score(y_test, y_test_pred),\n",
    "        'recall_score': recall_score(y_test, y_test_pred),\n",
    "        'f1_score': f1_score(y_test, y_test_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Déborah Leclercq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below function was created to retrieve lines from a website on a project called webscraping Manga.\n",
    "- You will need to import the following libraries:\n",
    "- import pandas as pd\n",
    "- import requests\n",
    "- from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the function to extract manga data from a given URL.\n",
    "def extract_manga_data(url):\n",
    "    for row in manga_rows:  # Loops through each row in 'manga_rows', which should contain individual manga entries.\n",
    "        \n",
    "        # Finds the title of the manga in the 'h3' tag with class 'lister-item-header', extracts the link text, and removes leading/trailing whitespace.\n",
    "        title = row.find(\"h3\", class_=\"lister-item-header\").a.text.strip()\n",
    "        \n",
    "        # Tries to find the rating element in a span with the class 'ipl-rating-star__rating'.\n",
    "        rating_element = row.find(\"span\", class_=\"ipl-rating-star__rating\")\n",
    "        \n",
    "        # Checks if the rating element exists. If so, extracts and cleans the rating text; otherwise, sets rating to None.\n",
    "        if rating_element:\n",
    "            rating = rating_element.text.strip()\n",
    "        else:\n",
    "            rating = None\n",
    "        \n",
    "        # Finds the release year of the manga, strips out parentheses from the 'lister-item-year' span text.\n",
    "        year = row.find(\"span\", class_=\"lister-item-year\").text.strip(\"()\")\n",
    "        \n",
    "        # Extracts the genre information from the 'genre' span and strips any extra whitespace.\n",
    "        genre = row.find(\"span\", class_=\"genre\").text.strip()\n",
    "        \n",
    "        # Appends a dictionary containing the manga title, year, rating, and genre to the 'manga_data' list.\n",
    "        manga_data.append({\"Title\": title, \"Year\": year, \"Rating\": rating, \"Genre\": genre})\n",
    "    \n",
    "    # Returns the list of dictionaries, each representing a manga with its details.\n",
    "    return manga_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Déborah Leclercq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This function requires Regular Expression Library:\n",
    "- Import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # v: str -> int (The function takes a string input and returns an integer)\n",
    "def ma_function_de_traitement(v): \n",
    "    import re  # Importing the 're' module to use regular expressions.\n",
    "    \n",
    "    # 'r' is a raw string pattern that matches any single character inside square brackets.\n",
    "    # The pattern \"\\[.\\]\" is used to match anything inside square brackets (e.g., \"[a]\").\n",
    "    r = r\"\\[.\\]\"\n",
    "    \n",
    "    v = str(v)  # Converts the input 'v' to a string, in case it isn't already a string.\n",
    "    \n",
    "    # 're.sub' is used to substitute any match of the pattern 'r' (i.e., anything in square brackets)\n",
    "    # with an empty string (i.e., it removes the matched part).\n",
    "    result = re.sub(r, \"\", v)\n",
    "    \n",
    "    # This pattern matches digits between 0 and 9 before a slash ('/'). The '+' ensures it matches \n",
    "    # one or more digits, and '/.*' captures the slash and any characters after it.\n",
    "    slash_paulette = r\"([0-9]+)/.*\"\n",
    "    \n",
    "    # 're.sub' replaces the part that matches 'slash_paulette' (digits followed by slash and more) \n",
    "    # with just the digits (first captured group). The result is cleaned of any unwanted characters like '\\x01'.\n",
    "    result = re.sub(slash_paulette, \"\\1\", result).replace('\\x01', '')\n",
    "    \n",
    "    # Converts the final result to an integer if the result is not empty. If empty, it returns 0.\n",
    "    return int(result) if result else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions created during Internship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Déborah Leclercq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function was needed during internship period to be able to \n",
    "isolate specific string information in a column so that is could be split at \n",
    "a specified wording type\n",
    "this requires \n",
    "import re - regular expression library\n",
    "import numpy as np - to help manage the nan values when there were some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re \n",
    "# regex for regular expressions - helps search for specific strings\n",
    "# import numpy as np # helps manage nan values\n",
    "\n",
    "def extract_phase(sentence):\n",
    "    '''this function will enable you to search for the location of a specific string value\n",
    "    \n",
    "    Input:\n",
    "        sentence (str): The string in which the function searches for the pattern.\n",
    "    \n",
    "    Output:\n",
    "        str or np.nan: \n",
    "            Returns the span of the matched pattern within the sentence, \n",
    "            represented as a tuple containing the start and end indexes. \n",
    "            Returns np.nan if the input sentence is NaN.\n",
    "    This will provide the span of the expression you were searching for aka locate the exact start and end indexes where the value\n",
    "    is located\n",
    "    \n",
    "    First you manage the Nan values if any are in your searching area and it returns the Nan \n",
    "    \n",
    "        expression : you need to feed the regex you are looking for\n",
    "    \n",
    "    check out the website https://regex101.com to feed your string and get the exact regex expression you need to \n",
    "    feed this variable\n",
    "    \n",
    "        pattern : the compilation of your expression to be able to search through your chosen column values\n",
    "    \n",
    "    re.search : using the search from regex to enable you to look for any match of your expression \n",
    "    https://docs.python.org/3/library/re.html#re.Pattern.search\n",
    "    \n",
    "    you can modify it to fit better your needs and visit above python library documentation if needed\n",
    "    \n",
    "    the pattern search specifies that you need the exact indexes of the expression you are searching\n",
    "    \n",
    "    the return feeds you the start index and end index of location of your expression\n",
    "\n",
    "    '''\n",
    "    if pd.isna(sentence):  # Check for NaN values\n",
    "        return np.nan\n",
    "    else:#the expression is regex equivalent to find any phase info or other string used to include in the new cols\n",
    "        expression = r\"Phase ['2\\/3''I\\/II''II\\/III''II\\-III'IV\\da|b]+|Preclinical ['planned'\\d]+|Inactive|Discontinued|X|Unknown|Preclinical|Discovery|Marketed|preclinical|In Vitro|Withdrawn|In vitro|x|\\?|Invitro\" \n",
    "        #regex expression to search for all types of phases as development stage\n",
    "        pattern = re.compile(expression)#creates the pattern to look for with regex\n",
    "        if re.search(pattern, sentence):#scan through string and look for any match from the pattern fed\n",
    "            start, end = pattern.search(sentence).span()#gets the span where the pattern is located \n",
    "            #aka 0 to 9 are the indexes where the pattern is situated eg result = re.Match object; span=(0, 1), match='d'>\n",
    "            return sentence[start:end]# returns the span location\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather several DFs in one excel doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Déborah Leclercq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a function but was a useful tool during internship to gather in one excel file several df created while cleaning and reformatting the file.\n",
    "- this requires import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code writes multiple DataFrames into an Excel file, with each DataFrame in a separate sheet.\n",
    "\n",
    "# Create an Excel writer object to write multiple sheets to one file.\n",
    "with pd.ExcelWriter(\"./output_data.xlsx\") as writer:  # Replace the file path with a generic name.\n",
    "    df_A.to_excel(writer, sheet_name='sheet_A', index=False)  # Write the first DataFrame to 'sheet_A'.\n",
    "    df_B.to_excel(writer, sheet_name='sheet_B', index=False)  # Write the second DataFrame to 'sheet_B'.\n",
    "    df_C.to_excel(writer, sheet_name='sheet_C', index=False)  # Write the third DataFrame to 'sheet_C'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find a specific column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Déborah Leclercq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a df with several cols and some are being repeated throughout and you need to identify the first one then this function will be useful\n",
    "- this requires pandas library - import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_comments_column(df):\n",
    "    for column in df.columns:\n",
    "        if column.startswith(\"Col_name_to_search\"):\n",
    "            return df.columns.get_loc(column)\n",
    "    # Si aucune colonne \"Comments\" n'est trouvée, retourner -1\n",
    "    return -1\n",
    "\n",
    "# Use this function to find the index of the first column name \"Comments\"\n",
    "index_comments = find_first_comments_column(df)\n",
    "print(\"First cols 'Comments' index is :\", index_comments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get the columns and their related indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Déborah Leclercq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function which will allow to display all cols and their related indexes\n",
    "- requires pandas library - import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all index and related cols\n",
    "def index_cols_info(df):\n",
    "    for i, column_name in enumerate(df.columns):\n",
    "        index_value = df.index[i]\n",
    "        print(f\"Index:{index_value}, Column: {column_name} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating a df at a specific column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Déborah Leclercq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "isolate each section of the df related to a separate selected part and call the subdf by a specific name\n",
    "- this was created because I need to slice my df at a particular section and needed to identify it\n",
    "- this requires pandas library - import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def slice_dataframe(df, start_indices, end_indices):\n",
    "    \"\"\"Slice DataFrame into sub-DataFrames using start and end indices.\n",
    "    Slice the DataFrame into sub-DataFrames based on the specified indices of \"Disease\" and \"Comments\" columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame to slice.\n",
    "    - start_indices: List of indices of \"Disease\" columns.\n",
    "    - end_indices: List of indices of \"Comments\" columns.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary containing the sliced sub-DataFrames with keys like 'df_1', 'df_2', etc.\n",
    "    \"\"\"\n",
    "    dfs = {}\n",
    "    for i, (start, end) in enumerate(zip(start_indices, end_indices)):\n",
    "        key = f'df_{df.columns[start]}'  # Use the first value of each selected column section as key\n",
    "        dfs[key] = df.iloc[:, start:end]\n",
    "    return dfs\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
